{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "725co5tCHyvd"
      },
      "source": [
        "#Initialize the first layer\r\n",
        "# probably helpful preliminaries\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import random\r\n",
        "import itertools"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6r13bCc2ZogQ"
      },
      "source": [
        "def activationFunction(x):\r\n",
        "  return np.tanh(x)\r\n",
        "def derivativeFunction(x):\r\n",
        "  return 1/(np.cosh(x) * np.cosh(x))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXgTA__rGxbm"
      },
      "source": [
        "#going forward and getting an output\r\n",
        "def goingForward(x,first_layer_weights,second_layer_weights):\r\n",
        "  Y_j = [] #second_layer values\r\n",
        "  for i in range(12):\r\n",
        "    Y_j.append(activationFunction(np.dot(first_layer_weights[i,:].T,x)))\r\n",
        "  Y_j=np.hstack((1,Y_j)) ##add a bias term\r\n",
        "  y = activationFunction(np.dot(second_layer_weights.T,Y_j))\r\n",
        "  return y, Y_j\r\n",
        "#goingForward(data[0],first_layer_weights,second_layer_weights)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmycBNkKd8_V"
      },
      "source": [
        "def backwardsPropegation(x,first_layer_weights,Y_j,second_layer_weights,y,d,learning_rate, momentum_ji,momentum_kj):\r\n",
        "#copied the previous weights to use for momentum \r\n",
        "  previous_update_ji = []\r\n",
        "  previous_update_kj = 0\r\n",
        "#weight update for W_kj\r\n",
        "  V_k = np.dot(second_layer_weights.T,Y_j)\r\n",
        "  error_k = (d-y)\r\n",
        "  #derivative of activation function\r\n",
        "  phi_prime_k = derivativeFunction(V_k)\r\n",
        "  delta_k = phi_prime_k * error_k\r\n",
        "  previous_update_kj = learning_rate*delta_k*Y_j\r\n",
        "  second_layer_weights = second_layer_weights + previous_update_kj + momentum_kj\r\n",
        "\r\n",
        "#weight update for W_ji\r\n",
        "  V_j = np.dot(first_layer_weights,x)\r\n",
        "  phi_prime_j = derivativeFunction(V_j)\r\n",
        "  delta_j = phi_prime_j * np.sum(second_layer_weights*delta_k) #shape 12x1\r\n",
        "  for i in range(len(first_layer_weights)):\r\n",
        "    update = learning_rate*delta_j[i]*x\r\n",
        "    previous_update_ji.append(update)\r\n",
        "    first_layer_weights[i] = first_layer_weights[i] + update + momentum_ji[i]\r\n",
        "\r\n",
        "#applying momentum for second layer\r\n",
        "  alpha = .8\r\n",
        "  momentum_kj = learning_rate*delta_k*Y_j*alpha\r\n",
        "#applying momentum for 1st layer\r\n",
        "  for i in range(len(first_layer_weights)):\r\n",
        "    momentum_ji[i] = delta_j[i]*learning_rate*x*alpha\r\n",
        "  return first_layer_weights, second_layer_weights, momentum_ji, momentum_kj\r\n",
        "\r\n",
        "#first_layer_weights, second_layer_weights=  backwardsPropegation(data[0],first_layer_weights,Y_j,second_layer_weights,y,d,.01)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kyvLamgPt5s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f836a45-d0a3-4a31-e273-da8034737a50"
      },
      "source": [
        "#creates all the data needed\r\n",
        "data = np.array([list(i) for i in itertools.product([0, 1], repeat=7)])\r\n",
        "for i in data:\r\n",
        "  for j in range(len(i)):\r\n",
        "    if(i[j]==0):\r\n",
        "      i[j] = -1\r\n",
        "#creates an array of random biases to add\r\n",
        "#this adds the biases to every row\r\n",
        "biases = []\r\n",
        "for i in range(128):\r\n",
        "  biases.append(1)\r\n",
        "data = np.column_stack((np.array(biases), data))\r\n",
        "d = []\r\n",
        "count = 0\r\n",
        "for i in data:\r\n",
        "  for j in range(len(i)):\r\n",
        "    if(i[j]==1):\r\n",
        "      count+=1\r\n",
        "  if (count%2==1):\r\n",
        "    d.append(1)\r\n",
        "  else:\r\n",
        "    d.append(-1)\r\n",
        "  count = 0\r\n",
        "\r\n",
        "#number of iterations\r\n",
        "eta=0.1\r\n",
        "learning_rate = [.005, .01, .015, .02, .025, .03, .035, .04, .045, .05]\r\n",
        "for k in range(len(learning_rate)):\r\n",
        "  \r\n",
        "  converged=False\r\n",
        "  epoch=0\r\n",
        "  #first layer weights\r\n",
        "  first_layer_weights =  np.random.uniform(-1, 1, (12,8))\r\n",
        "  #from 1st layer to outputs 13->1, first value is for the bias\r\n",
        "  second_layer_weights =  np.random.uniform(-1, 1, (13,1)).reshape(13)\r\n",
        "  momentum_ji = [0,0,0,0,0,0,0,0,0,0,0,0]\r\n",
        "  momentum_kj = 0\r\n",
        "  while not converged:\r\n",
        "    average_error = 0\r\n",
        "    epoch+=1\r\n",
        "    for i in range(len(data)):\r\n",
        "      y, Y_j = goingForward(data[i], first_layer_weights, second_layer_weights) \r\n",
        "      first_layer_weights, second_layer_weights, momentum_ji, momentum_kj = backwardsPropegation(data[i],first_layer_weights,np.array(Y_j),\\\r\n",
        "                                                                       second_layer_weights,y,d[i],learning_rate[k],momentum_ji,momentum_kj)\r\n",
        "      average_error += abs(d[i]-y)\r\n",
        "    average_error = average_error/len(data)\r\n",
        "    converged=average_error<eta\r\n",
        "    #print(average_error)\r\n",
        "  print(\"Learning Rate: \", learning_rate[k])\r\n",
        "  print(\"Converged at epoch \"+str(epoch))\r\n",
        "  print()\r\n",
        "  \r\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning Rate:  0.005\n",
            "Converged at epoch 68\n",
            "\n",
            "Learning Rate:  0.01\n",
            "Converged at epoch 673\n",
            "\n",
            "Learning Rate:  0.015\n",
            "Converged at epoch 26\n",
            "\n",
            "Learning Rate:  0.02\n",
            "Converged at epoch 35\n",
            "\n",
            "Learning Rate:  0.025\n",
            "Converged at epoch 19\n",
            "\n",
            "Learning Rate:  0.03\n",
            "Converged at epoch 28\n",
            "\n",
            "Learning Rate:  0.035\n",
            "Converged at epoch 65\n",
            "\n",
            "Learning Rate:  0.04\n",
            "Converged at epoch 31\n",
            "\n",
            "Learning Rate:  0.045\n",
            "Converged at epoch 19\n",
            "\n",
            "Learning Rate:  0.05\n",
            "Converged at epoch 23\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}