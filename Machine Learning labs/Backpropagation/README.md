# Backpropagation on a multi-layer network from scratch

### The objective of this lab was to fulling understand how assigning blame works in backpropagation.
### From previous models, we always assigned the update to our weights going forward. (Linear regression, Logistic regression, MLP)
### Yet, depending on the problem, especially in deep neural networks, it works better to use backpropagation.
### For this lab, we focused primarily on understanding the algorithm and find the convergence for each learning rate according to number of epochs.
### We were to also implement momentum and observe the impact in our network.
